# 13. ìŠ¤ì¼€ì¼ë§ ì „ëµ

## ğŸ¯ ëª©í‘œ
LogCasterë¥¼ ìˆ˜í‰/ìˆ˜ì§ìœ¼ë¡œ í™•ì¥í•˜ì—¬ ì¦ê°€í•˜ëŠ” íŠ¸ë˜í”½ê³¼ ë°ì´í„°ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ ì „ëµì…ë‹ˆë‹¤.

## ğŸ“‹ Prerequisites
- ë¡œë“œ ë°¸ëŸ°ì„œ êµ¬ì„±
- ì»¨í…Œì´ë„ˆ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ (Kubernetes)
- ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ
- ìë™ ìŠ¤ì¼€ì¼ë§ ë„êµ¬

---

## 1. ìŠ¤ì¼€ì¼ë§ ì•„í‚¤í…ì²˜

### 1.1 ìˆ˜í‰ í™•ì¥ (Horizontal Scaling)

```yaml
# horizontal-scaling-architecture.yaml
architecture:
  load_balancer:
    type: nginx/haproxy
    strategy: least_connections
    health_check_interval: 5s
    
  application_tier:
    min_instances: 3
    max_instances: 20
    target_cpu: 70%
    target_memory: 80%
    
  data_tier:
    sharding_strategy: consistent_hashing
    replication_factor: 3
    partition_count: 128
```

### 1.2 ìˆ˜ì§ í™•ì¥ (Vertical Scaling)

```yaml
# vertical-scaling-tiers.yaml
instance_types:
  small:
    cpu: 2
    memory: 4GB
    network: 1Gbps
    iops: 3000
    
  medium:
    cpu: 4
    memory: 16GB
    network: 5Gbps
    iops: 10000
    
  large:
    cpu: 16
    memory: 64GB
    network: 10Gbps
    iops: 30000
    
  xlarge:
    cpu: 32
    memory: 128GB
    network: 25Gbps
    iops: 60000
```

---

## 2. Kubernetes ìë™ ìŠ¤ì¼€ì¼ë§

### 2.1 Horizontal Pod Autoscaler (HPA)

```yaml
# hpa-logcaster.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: logcaster-hpa
  namespace: production
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: logcaster
  minReplicas: 3
  maxReplicas: 50
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  - type: Pods
    pods:
      metric:
        name: logcaster_active_connections
      target:
        type: AverageValue
        averageValue: "100"
  - type: External
    external:
      metric:
        name: queue_size
        selector:
          matchLabels:
            queue: logs
      target:
        type: Value
        value: "1000"
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
      - type: Pods
        value: 2
        periodSeconds: 60
      selectPolicy: Min
    scaleUp:
      stabilizationWindowSeconds: 0
      policies:
      - type: Percent
        value: 100
        periodSeconds: 15
      - type: Pods
        value: 5
        periodSeconds: 15
      selectPolicy: Max
```

### 2.2 Vertical Pod Autoscaler (VPA)

```yaml
# vpa-logcaster.yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: logcaster-vpa
  namespace: production
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: logcaster
  updatePolicy:
    updateMode: "Auto"  # Off, Initial, Auto
  resourcePolicy:
    containerPolicies:
    - containerName: logcaster
      minAllowed:
        cpu: 100m
        memory: 128Mi
      maxAllowed:
        cpu: 4
        memory: 8Gi
      controlledResources: ["cpu", "memory"]
```

### 2.3 Cluster Autoscaler

```yaml
# cluster-autoscaler.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cluster-autoscaler
  namespace: kube-system
spec:
  template:
    spec:
      containers:
      - image: k8s.gcr.io/autoscaling/cluster-autoscaler:v1.27.0
        name: cluster-autoscaler
        command:
        - ./cluster-autoscaler
        - --v=4
        - --stderrthreshold=info
        - --cloud-provider=aws
        - --skip-nodes-with-local-storage=false
        - --expander=least-waste
        - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/logcaster-cluster
        - --balance-similar-node-groups
        - --skip-nodes-with-system-pods=false
        env:
        - name: AWS_REGION
          value: ap-northeast-2
```

---

## 3. ë¡œë“œ ë°¸ëŸ°ì‹± ì „ëµ

### 3.1 NGINX ë¡œë“œ ë°¸ëŸ°ì„œ

```nginx
# nginx-load-balancer.conf
upstream logcaster_backend {
    least_conn;  # ë˜ëŠ” ip_hash, random
    
    server backend1.example.com:9999 weight=3 max_fails=3 fail_timeout=30s;
    server backend2.example.com:9999 weight=2 max_fails=3 fail_timeout=30s;
    server backend3.example.com:9999 weight=1 max_fails=3 fail_timeout=30s;
    
    # ë°±ì—… ì„œë²„
    server backup1.example.com:9999 backup;
    
    # í—¬ìŠ¤ì²´í¬
    keepalive 32;
    keepalive_requests 100;
    keepalive_timeout 60s;
}

server {
    listen 80;
    server_name logcaster.example.com;
    
    location / {
        proxy_pass http://logcaster_backend;
        proxy_next_upstream error timeout http_500 http_502 http_503;
        proxy_connect_timeout 1s;
        proxy_send_timeout 30s;
        proxy_read_timeout 30s;
        
        # ì—°ê²° ì¬ì‚¬ìš©
        proxy_http_version 1.1;
        proxy_set_header Connection "";
        
        # í—¤ë” ì„¤ì •
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        
        # ë²„í¼ë§
        proxy_buffering on;
        proxy_buffer_size 4k;
        proxy_buffers 8 4k;
        proxy_busy_buffers_size 8k;
    }
    
    # í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸
    location /health {
        access_log off;
        return 200 "healthy\n";
        add_header Content-Type text/plain;
    }
}
```

### 3.2 HAProxy ë¡œë“œ ë°¸ëŸ°ì„œ

```conf
# haproxy.cfg
global
    maxconn 50000
    log stdout local0
    nbproc 4
    nbthread 4
    cpu-map auto:1/1-4 0-3

defaults
    mode tcp
    timeout connect 5s
    timeout client 30s
    timeout server 30s
    option tcplog

frontend logcaster_frontend
    bind *:9999
    mode tcp
    default_backend logcaster_backend

backend logcaster_backend
    mode tcp
    balance leastconn
    
    # í—¬ìŠ¤ì²´í¬
    option tcp-check
    tcp-check connect
    tcp-check send "PING\r\n"
    tcp-check expect string "PONG"
    
    # ì„œë²„ í’€
    server backend1 10.0.1.10:9999 check inter 2s rise 2 fall 3 weight 100
    server backend2 10.0.1.11:9999 check inter 2s rise 2 fall 3 weight 100
    server backend3 10.0.1.12:9999 check inter 2s rise 2 fall 3 weight 100
    
    # ë™ì  ì„œë²„ ì¶”ê°€
    server-template srv 1-10 _backend._tcp.service.consul resolvers consul resolve-opts allow-dup-ip resolve-prefer ipv4 check
```

---

## 4. ë°ì´í„° ìƒ¤ë”©

### 4.1 ìƒ¤ë”© êµ¬í˜„

```c
// sharding.c
#include <stdint.h>
#include <string.h>

// Consistent Hashingì„ ìœ„í•œ êµ¬ì¡°ì²´
typedef struct {
    uint32_t hash;
    int server_id;
} hash_node_t;

typedef struct {
    hash_node_t* nodes;
    int node_count;
    int server_count;
    int virtual_nodes;  // ê°€ìƒ ë…¸ë“œ ìˆ˜
} consistent_hash_t;

// MurmurHash3
uint32_t murmur3_32(const uint8_t* key, size_t len, uint32_t seed) {
    const uint32_t c1 = 0xcc9e2d51;
    const uint32_t c2 = 0x1b873593;
    const uint32_t r1 = 15;
    const uint32_t r2 = 13;
    const uint32_t m = 5;
    const uint32_t n = 0xe6546b64;
    
    uint32_t hash = seed;
    const int nblocks = len / 4;
    const uint32_t* blocks = (const uint32_t*)key;
    
    for (int i = 0; i < nblocks; i++) {
        uint32_t k = blocks[i];
        k *= c1;
        k = (k << r1) | (k >> (32 - r1));
        k *= c2;
        
        hash ^= k;
        hash = ((hash << r2) | (hash >> (32 - r2))) * m + n;
    }
    
    const uint8_t* tail = (const uint8_t*)(key + nblocks * 4);
    uint32_t k1 = 0;
    
    switch (len & 3) {
        case 3: k1 ^= tail[2] << 16;
        case 2: k1 ^= tail[1] << 8;
        case 1: k1 ^= tail[0];
            k1 *= c1;
            k1 = (k1 << r1) | (k1 >> (32 - r1));
            k1 *= c2;
            hash ^= k1;
    }
    
    hash ^= len;
    hash ^= (hash >> 16);
    hash *= 0x85ebca6b;
    hash ^= (hash >> 13);
    hash *= 0xc2b2ae35;
    hash ^= (hash >> 16);
    
    return hash;
}

// ìƒ¤ë“œ ì„ íƒ
int get_shard(consistent_hash_t* ch, const char* key) {
    uint32_t hash = murmur3_32((uint8_t*)key, strlen(key), 0);
    
    // Binary search for the right node
    int left = 0, right = ch->node_count - 1;
    while (left < right) {
        int mid = left + (right - left) / 2;
        if (ch->nodes[mid].hash < hash) {
            left = mid + 1;
        } else {
            right = mid;
        }
    }
    
    // Wrap around if necessary
    if (left == ch->node_count) {
        left = 0;
    }
    
    return ch->nodes[left].server_id;
}

// ìƒ¤ë“œ ì¬ë¶„ë°°
void rebalance_shards(consistent_hash_t* ch, int new_server_id) {
    // ìƒˆ ì„œë²„ë¥¼ ìœ„í•œ ê°€ìƒ ë…¸ë“œ ì¶”ê°€
    for (int i = 0; i < ch->virtual_nodes; i++) {
        char vnode[64];
        snprintf(vnode, sizeof(vnode), "server%d-%d", new_server_id, i);
        uint32_t hash = murmur3_32((uint8_t*)vnode, strlen(vnode), 0);
        
        // ì •ë ¬ëœ ìœ„ì¹˜ì— ì‚½ì…
        // ... (ì‚½ì… ë¡œì§)
    }
}
```

### 4.2 ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜

```bash
#!/bin/bash
# data-migration.sh

echo "ğŸ“¦ ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì‘..."

OLD_SHARD=$1
NEW_SHARD=$2
KEY_RANGE_START=$3
KEY_RANGE_END=$4

# 1. ìƒˆ ìƒ¤ë“œ ì¤€ë¹„
ssh $NEW_SHARD "mkdir -p /var/lib/logcaster/data"

# 2. ë°ì´í„° ë³µì‚¬ (ì˜¨ë¼ì¸)
redis-cli --scan --pattern "log:*" | while read key; do
    HASH=$(echo -n "$key" | md5sum | cut -d' ' -f1)
    if [[ "$HASH" > "$KEY_RANGE_START" && "$HASH" < "$KEY_RANGE_END" ]]; then
        # ë°ì´í„° ì´ë™
        redis-cli --raw dump "$key" | head -c-1 | \
            redis-cli -h $NEW_SHARD --pipe
    fi
done

# 3. ê²€ì¦
COUNT_OLD=$(redis-cli -h $OLD_SHARD dbsize | awk '{print $1}')
COUNT_NEW=$(redis-cli -h $NEW_SHARD dbsize | awk '{print $1}')
echo "ì´ë™ëœ í‚¤: $COUNT_NEW"

# 4. ë¼ìš°íŒ… í…Œì´ë¸” ì—…ë°ì´íŠ¸
curl -X POST http://config-server/api/shards \
    -d "{\"shard\": \"$NEW_SHARD\", \"range\": [\"$KEY_RANGE_START\", \"$KEY_RANGE_END\"]}"

echo "âœ… ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ"
```

---

## 5. ìºì‹± ì „ëµ

### 5.1 ë‹¤ì¸µ ìºì‹±

```c
// multi_tier_cache.c
typedef struct {
    // L1: ë¡œì»¬ ë©”ëª¨ë¦¬ ìºì‹œ
    struct {
        void* data;
        size_t size;
        time_t expiry;
    } l1_cache[1000];
    
    // L2: Redis ìºì‹œ
    redisContext* redis_conn;
    
    // L3: CDN ìºì‹œ
    char* cdn_endpoint;
} multi_tier_cache_t;

void* get_from_cache(multi_tier_cache_t* cache, const char* key) {
    // L1 ìºì‹œ í™•ì¸
    uint32_t hash = hash_function(key) % 1000;
    if (cache->l1_cache[hash].data && 
        cache->l1_cache[hash].expiry > time(NULL)) {
        return cache->l1_cache[hash].data;
    }
    
    // L2 ìºì‹œ í™•ì¸
    redisReply* reply = redisCommand(cache->redis_conn, "GET %s", key);
    if (reply && reply->type == REDIS_REPLY_STRING) {
        // L1 ìºì‹œì— ì €ì¥
        cache->l1_cache[hash].data = strdup(reply->str);
        cache->l1_cache[hash].expiry = time(NULL) + 60;
        
        freeReplyObject(reply);
        return cache->l1_cache[hash].data;
    }
    
    // L3 ìºì‹œ (CDN) í™•ì¸
    // ... CDN API í˜¸ì¶œ
    
    return NULL;
}
```

### 5.2 Redis Cluster ì„¤ì •

```bash
#!/bin/bash
# setup-redis-cluster.sh

# Redis ë…¸ë“œ ì‹œì‘
for port in 7000 7001 7002 7003 7004 7005; do
    mkdir -p /var/lib/redis/$port
    cat > /var/lib/redis/$port/redis.conf <<EOF
port $port
cluster-enabled yes
cluster-config-file nodes-$port.conf
cluster-node-timeout 5000
appendonly yes
maxmemory 2gb
maxmemory-policy allkeys-lru
EOF
    redis-server /var/lib/redis/$port/redis.conf &
done

sleep 5

# í´ëŸ¬ìŠ¤í„° ìƒì„±
redis-cli --cluster create \
    127.0.0.1:7000 127.0.0.1:7001 \
    127.0.0.1:7002 127.0.0.1:7003 \
    127.0.0.1:7004 127.0.0.1:7005 \
    --cluster-replicas 1 \
    --cluster-yes

echo "âœ… Redis í´ëŸ¬ìŠ¤í„° êµ¬ì„± ì™„ë£Œ"
```

---

## 6. ìë™ ìŠ¤ì¼€ì¼ë§ ì •ì±…

### 6.1 ë©”íŠ¸ë¦­ ê¸°ë°˜ ìŠ¤ì¼€ì¼ë§

```python
# autoscaling_policy.py
import boto3
import time
from datetime import datetime, timedelta

class AutoScaler:
    def __init__(self):
        self.cloudwatch = boto3.client('cloudwatch')
        self.autoscaling = boto3.client('autoscaling')
        self.asg_name = 'logcaster-asg'
        
    def get_metrics(self):
        """ìµœê·¼ 5ë¶„ê°„ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        end_time = datetime.utcnow()
        start_time = end_time - timedelta(minutes=5)
        
        response = self.cloudwatch.get_metric_statistics(
            Namespace='LogCaster',
            MetricName='ActiveConnections',
            Dimensions=[{'Name': 'ServiceName', 'Value': 'logcaster'}],
            StartTime=start_time,
            EndTime=end_time,
            Period=300,
            Statistics=['Average'],
            Unit='Count'
        )
        
        if response['Datapoints']:
            return response['Datapoints'][0]['Average']
        return 0
    
    def scale_decision(self, current_metric):
        """ìŠ¤ì¼€ì¼ë§ ê²°ì •"""
        # í˜„ì¬ ì¸ìŠ¤í„´ìŠ¤ ìˆ˜
        response = self.autoscaling.describe_auto_scaling_groups(
            AutoScalingGroupNames=[self.asg_name]
        )
        current_capacity = response['AutoScalingGroups'][0]['DesiredCapacity']
        
        # ìŠ¤ì¼€ì¼ë§ ê·œì¹™
        connections_per_instance = 1000
        required_instances = max(3, int(current_metric / connections_per_instance) + 1)
        
        if required_instances > current_capacity * 1.2:
            # Scale up
            new_capacity = min(50, current_capacity + 2)
            self.scale_to(new_capacity)
            return f"Scaled up to {new_capacity} instances"
            
        elif required_instances < current_capacity * 0.6:
            # Scale down
            new_capacity = max(3, current_capacity - 1)
            self.scale_to(new_capacity)
            return f"Scaled down to {new_capacity} instances"
            
        return "No scaling needed"
    
    def scale_to(self, capacity):
        """ì¸ìŠ¤í„´ìŠ¤ ìˆ˜ ì¡°ì •"""
        self.autoscaling.set_desired_capacity(
            AutoScalingGroupName=self.asg_name,
            DesiredCapacity=capacity,
            HonorCooldown=True
        )
    
    def predictive_scaling(self):
        """ì˜ˆì¸¡ ê¸°ë°˜ ìŠ¤ì¼€ì¼ë§"""
        # ê³¼ê±° íŒ¨í„´ ë¶„ì„
        historical_data = self.get_historical_patterns()
        
        # ì‹œê°„ëŒ€ë³„ ì˜ˆì¸¡
        current_hour = datetime.now().hour
        if current_hour in [9, 10, 11, 14, 15, 16]:  # í”¼í¬ ì‹œê°„
            self.scale_to(10)
        elif current_hour in [0, 1, 2, 3, 4, 5]:  # ì €ì¡°í•œ ì‹œê°„
            self.scale_to(3)
        else:
            self.scale_to(5)

# ì‹¤í–‰
if __name__ == "__main__":
    scaler = AutoScaler()
    
    while True:
        metric = scaler.get_metrics()
        result = scaler.scale_decision(metric)
        print(f"[{datetime.now()}] Metric: {metric}, Action: {result}")
        time.sleep(60)
```

### 6.2 ë¹„ìš© ìµœì í™” ìŠ¤ì¼€ì¼ë§

```bash
#!/bin/bash
# cost-optimized-scaling.sh

# Spot ì¸ìŠ¤í„´ìŠ¤ í˜¼í•© ì‚¬ìš©
aws autoscaling update-auto-scaling-group \
    --auto-scaling-group-name logcaster-asg \
    --mixed-instances-policy '{
        "LaunchTemplate": {
            "LaunchTemplateSpecification": {
                "LaunchTemplateName": "logcaster-template",
                "Version": "$Latest"
            },
            "Overrides": [
                {"InstanceType": "t3.medium"},
                {"InstanceType": "t3a.medium"},
                {"InstanceType": "t2.medium"}
            ]
        },
        "InstancesDistribution": {
            "OnDemandAllocationStrategy": "prioritized",
            "OnDemandBaseCapacity": 2,
            "OnDemandPercentageAboveBaseCapacity": 30,
            "SpotAllocationStrategy": "lowest-price",
            "SpotInstancePools": 3
        }
    }'

# ì˜ˆì•½ ì¸ìŠ¤í„´ìŠ¤ í™œìš©
aws ec2 purchase-reserved-instances-offering \
    --reserved-instances-offering-id offering-id \
    --instance-count 3

echo "âœ… ë¹„ìš© ìµœì í™” ìŠ¤ì¼€ì¼ë§ ì„¤ì • ì™„ë£Œ"
```

---

## 7. ê¸€ë¡œë²Œ ìŠ¤ì¼€ì¼ë§

### 7.1 ì§€ì—­ë³„ ë°°í¬

```yaml
# global-deployment.yaml
regions:
  us-east-1:
    instances: 5
    load_balancer: alb-us-east
    database: rds-us-east
    cache: elasticache-us-east
    
  eu-west-1:
    instances: 3
    load_balancer: alb-eu-west
    database: rds-eu-west
    cache: elasticache-eu-west
    
  ap-northeast-1:
    instances: 4
    load_balancer: alb-ap-northeast
    database: rds-ap-northeast
    cache: elasticache-ap-northeast

cross_region_replication:
  enabled: true
  consistency: eventual
  lag_threshold: 1000ms
```

### 7.2 GeoDNS ì„¤ì •

```bash
# Route53 GeoDNS ì„¤ì •
aws route53 change-resource-record-sets \
    --hosted-zone-id Z123456789 \
    --change-batch '{
        "Changes": [{
            "Action": "CREATE",
            "ResourceRecordSet": {
                "Name": "logcaster.example.com",
                "Type": "A",
                "SetIdentifier": "US",
                "GeoLocation": {
                    "CountryCode": "US"
                },
                "AliasTarget": {
                    "HostedZoneId": "Z215JYRZR8TBV",
                    "DNSName": "us-alb.example.com",
                    "EvaluateTargetHealth": true
                }
            }
        }]
    }'
```

---

## 8. ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼

### 8.1 ìŠ¤ì¼€ì¼ë§ ì´ë²¤íŠ¸ ëª¨ë‹ˆí„°ë§

```yaml
# scaling-alerts.yaml
alerts:
  - name: ScalingFrequent
    condition: count(scaling_events) > 10 in 1h
    action: notify_ops
    
  - name: ScalingFailed
    condition: scaling_status == "failed"
    action: page_oncall
    
  - name: CapacityLimit
    condition: current_instances >= max_instances * 0.9
    action: alert_capacity_planning
```

### 8.2 ìŠ¤ì¼€ì¼ë§ ëŒ€ì‹œë³´ë“œ

```json
{
  "dashboard": {
    "title": "LogCaster Scaling Dashboard",
    "panels": [
      {
        "title": "Instance Count",
        "query": "sum(up{job='logcaster'})"
      },
      {
        "title": "Scaling Events",
        "query": "rate(autoscaling_events_total[5m])"
      },
      {
        "title": "Cost Projection",
        "query": "instance_count * instance_cost_per_hour * 24 * 30"
      }
    ]
  }
}
```

---

## âœ… ì²´í¬ë¦¬ìŠ¤íŠ¸

### í•„ìˆ˜ í™•ì¸ì‚¬í•­
- [ ] HPA/VPA êµ¬ì„±
- [ ] ë¡œë“œ ë°¸ëŸ°ì„œ ì„¤ì •
- [ ] ìë™ ìŠ¤ì¼€ì¼ë§ ì •ì±…
- [ ] ëª¨ë‹ˆí„°ë§ ë©”íŠ¸ë¦­
- [ ] ë¹„ìš© ìµœì í™” ì „ëµ
- [ ] ì¥ì•  ë³µêµ¬ ê³„íš

### ê¶Œì¥ì‚¬í•­
- [ ] ì˜ˆì¸¡ ê¸°ë°˜ ìŠ¤ì¼€ì¼ë§
- [ ] ë©€í‹° ë¦¬ì „ ë°°í¬
- [ ] ìºì‹± ê³„ì¸µ êµ¬ì„±
- [ ] ë°ì´í„° ìƒ¤ë”©
- [ ] Spot ì¸ìŠ¤í„´ìŠ¤ í™œìš©
- [ ] ê¸€ë¡œë²Œ ë¡œë“œ ë°¸ëŸ°ì‹±

---

## ğŸ¯ ì™„ë£Œ
ì´ì œ LogCasterì˜ í¬ê´„ì ì¸ ë°°í¬ ê°€ì´ë“œê°€ ì™„ì„±ë˜ì—ˆìŠµë‹ˆë‹¤!