# 06. ë¡œê¹… ì‹œìŠ¤í…œ ì„¤ì •

## ğŸ¯ ëª©í‘œ
LogCasterì˜ êµ¬ì¡°í™”ëœ ë¡œê¹… ì‹œìŠ¤í…œì„ êµ¬ì¶•í•˜ê³  ì¤‘ì•™ ì§‘ì¤‘ì‹ ë¡œê·¸ ê´€ë¦¬ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.

## ğŸ“‹ Prerequisites
- Elasticsearch 7.x+ ë˜ëŠ” Loki
- Fluentd ë˜ëŠ” Filebeat
- ì¶©ë¶„í•œ ìŠ¤í† ë¦¬ì§€ (ìµœì†Œ 100GB)
- ë¡œê·¸ ë³´ì¡´ ì •ì±… ê²°ì •

---

## 1. ë¡œê¹… ì•„í‚¤í…ì²˜

### 1.1 ë¡œê·¸ ë ˆë²¨ ì •ì˜

```c
// log_levels.h
typedef enum {
    LOG_TRACE = 0,    // ìƒì„¸ ë””ë²„ê¹… ì •ë³´
    LOG_DEBUG = 1,    // ë””ë²„ê¹… ì •ë³´
    LOG_INFO = 2,     // ì¼ë°˜ ì •ë³´
    LOG_WARN = 3,     // ê²½ê³ 
    LOG_ERROR = 4,    // ì—ëŸ¬
    LOG_FATAL = 5     // ì¹˜ëª…ì  ì—ëŸ¬
} log_level_t;

// êµ¬ì¡°í™”ëœ ë¡œê·¸ í¬ë§·
typedef struct {
    time_t timestamp;
    log_level_t level;
    char* component;
    char* message;
    char* trace_id;
    char* user_id;
    char* client_ip;
    int response_time_ms;
    char* extra_json;
} log_entry_t;
```

### 1.2 ë¡œê·¸ ì¶œë ¥ êµ¬í˜„

```c
// structured_logger.c
#include <json-c/json.h>

void log_structured(log_entry_t* entry) {
    json_object* jobj = json_object_new_object();
    
    // íƒ€ì„ìŠ¤íƒ¬í”„
    char timestamp[64];
    strftime(timestamp, sizeof(timestamp), "%Y-%m-%dT%H:%M:%S%z", 
             localtime(&entry->timestamp));
    json_object_object_add(jobj, "@timestamp", 
                           json_object_new_string(timestamp));
    
    // ë¡œê·¸ ë ˆë²¨
    const char* level_str[] = {"TRACE", "DEBUG", "INFO", "WARN", "ERROR", "FATAL"};
    json_object_object_add(jobj, "level", 
                           json_object_new_string(level_str[entry->level]));
    
    // ì»´í¬ë„ŒíŠ¸
    json_object_object_add(jobj, "component", 
                           json_object_new_string(entry->component));
    
    // ë©”ì‹œì§€
    json_object_object_add(jobj, "message", 
                           json_object_new_string(entry->message));
    
    // ì¶”ì  ì •ë³´
    if (entry->trace_id) {
        json_object_object_add(jobj, "trace_id", 
                               json_object_new_string(entry->trace_id));
    }
    
    // ì‚¬ìš©ì ì •ë³´
    if (entry->user_id) {
        json_object_object_add(jobj, "user_id", 
                               json_object_new_string(entry->user_id));
    }
    
    // í´ë¼ì´ì–¸íŠ¸ IP
    if (entry->client_ip) {
        json_object_object_add(jobj, "client_ip", 
                               json_object_new_string(entry->client_ip));
    }
    
    // ì‘ë‹µ ì‹œê°„
    if (entry->response_time_ms > 0) {
        json_object_object_add(jobj, "response_time_ms", 
                               json_object_new_int(entry->response_time_ms));
    }
    
    // JSON ì¶œë ¥
    fprintf(log_file, "%s\n", json_object_to_json_string_ext(jobj, 
            JSON_C_TO_STRING_PLAIN));
    fflush(log_file);
    
    json_object_put(jobj);
}
```

---

## 2. Fluentd ì„¤ì •

### 2.1 Fluentd ì„¤ì¹˜ ë° êµ¬ì„±

```dockerfile
# Dockerfile.fluentd
FROM fluent/fluentd:v1.16-debian

USER root

# í”ŒëŸ¬ê·¸ì¸ ì„¤ì¹˜
RUN gem install fluent-plugin-elasticsearch \
                fluent-plugin-prometheus \
                fluent-plugin-s3 \
                fluent-plugin-kafka

USER fluent
```

### 2.2 Fluentd ì„¤ì • íŒŒì¼

```ruby
# fluent.conf
# LogCaster ë¡œê·¸ ìˆ˜ì§‘
<source>
  @type tail
  path /var/log/logcaster/*.log
  pos_file /var/log/fluentd/logcaster.pos
  tag logcaster.*
  <parse>
    @type json
    time_key @timestamp
    time_format %Y-%m-%dT%H:%M:%S%z
  </parse>
</source>

# ë¡œê·¸ í•„í„°ë§ ë° ë³€í™˜
<filter logcaster.**>
  @type record_transformer
  <record>
    hostname ${hostname}
    environment ${ENV}
    service logcaster
  </record>
</filter>

# ì—ëŸ¬ ë¡œê·¸ íŠ¹ë³„ ì²˜ë¦¬
<filter logcaster.**>
  @type grep
  <regexp>
    key level
    pattern ERROR|FATAL
  </regexp>
</filter>

# Elasticsearch ì¶œë ¥
<match logcaster.**>
  @type elasticsearch
  host elasticsearch
  port 9200
  logstash_format true
  logstash_prefix logcaster
  logstash_dateformat %Y.%m.%d
  include_timestamp true
  <buffer>
    @type file
    path /var/log/fluentd/buffer/elasticsearch
    flush_interval 10s
    chunk_limit_size 5M
    retry_limit 5
    retry_wait 10s
  </buffer>
</match>

# S3 ë°±ì—… (ì„ íƒì‚¬í•­)
<match logcaster.**>
  @type s3
  aws_key_id YOUR_AWS_KEY_ID
  aws_sec_key YOUR_AWS_SECRET_KEY
  s3_bucket logcaster-logs
  s3_region ap-northeast-2
  path logs/%Y/%m/%d/
  <buffer time>
    @type file
    path /var/log/fluentd/buffer/s3
    timekey 3600  # 1ì‹œê°„ë§ˆë‹¤
    timekey_wait 10m
    chunk_limit_size 256m
  </buffer>
</match>

# ë©”íŠ¸ë¦­ ìˆ˜ì§‘
<source>
  @type prometheus
  bind 0.0.0.0
  port 24231
  metrics_path /metrics
</source>

<source>
  @type prometheus_monitor
</source>
```

---

## 3. Filebeat ëŒ€ì•ˆ ì„¤ì •

### 3.1 Filebeat êµ¬ì„±

```yaml
# filebeat.yml
filebeat.inputs:
- type: log
  enabled: true
  paths:
    - /var/log/logcaster/*.log
  json.keys_under_root: true
  json.add_error_key: true
  fields:
    service: logcaster
    environment: ${ENV:production}
  fields_under_root: true

- type: log
  enabled: true
  paths:
    - /var/log/logcaster/error.log
  multiline.pattern: '^\d{4}-\d{2}-\d{2}'
  multiline.negate: true
  multiline.match: after
  fields:
    log_type: error

processors:
  - add_host_metadata:
      when.not.contains:
        tags: forwarded
  - add_docker_metadata: ~
  - add_kubernetes_metadata: ~
  - drop_event:
      when:
        or:
          - regexp:
              message: "^DEBUG"
          - contains:
              message: "health_check"

output.elasticsearch:
  hosts: ["elasticsearch:9200"]
  index: "logcaster-%{+yyyy.MM.dd}"
  template.name: "logcaster"
  template.pattern: "logcaster-*"
  template.settings:
    index.number_of_shards: 2
    index.number_of_replicas: 1

logging.level: info
logging.to_files: true
logging.files:
  path: /var/log/filebeat
  name: filebeat
  keepfiles: 7
  permissions: 0640
```

---

## 4. ì¤‘ì•™ ì§‘ì¤‘ì‹ ë¡œê¹… (Loki)

### 4.1 Loki ìŠ¤íƒ ì„¤ì •

```yaml
# docker-compose-loki.yml
version: '3.8'

services:
  loki:
    image: grafana/loki:2.9.0
    ports:
      - "3100:3100"
    command: -config.file=/etc/loki/local-config.yaml
    volumes:
      - ./loki-config.yaml:/etc/loki/local-config.yaml
      - loki_data:/loki
    networks:
      - loki

  promtail:
    image: grafana/promtail:2.9.0
    volumes:
      - /var/log:/var/log:ro
      - ./promtail-config.yaml:/etc/promtail/config.yml
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
    command: -config.file=/etc/promtail/config.yml
    networks:
      - loki

  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana-datasources.yaml:/etc/grafana/provisioning/datasources/datasources.yaml
    networks:
      - loki

volumes:
  loki_data:
  grafana_data:

networks:
  loki:
    driver: bridge
```

### 4.2 Promtail ì„¤ì •

```yaml
# promtail-config.yaml
server:
  http_listen_port: 9080
  grpc_listen_port: 0

positions:
  filename: /tmp/positions.yaml

clients:
  - url: http://loki:3100/loki/api/v1/push

scrape_configs:
  - job_name: logcaster
    static_configs:
      - targets:
          - localhost
        labels:
          job: logcaster
          __path__: /var/log/logcaster/*.log
    pipeline_stages:
      - json:
          expressions:
            level: level
            component: component
            message: message
            timestamp: "@timestamp"
      - labels:
          level:
          component:
      - timestamp:
          source: timestamp
          format: "2006-01-02T15:04:05Z07:00"
      - metrics:
          log_lines_total:
            type: Counter
            description: "Total number of log lines"
            source: level
            config:
              action: inc
```

---

## 5. ë¡œê·¸ ë¶„ì„ ë° ì•Œë¦¼

### 5.1 ë¡œê·¸ ë¶„ì„ ê·œì¹™

```yaml
# log-analysis-rules.yaml
rules:
  - name: high_error_rate
    query: |
      sum(rate({job="logcaster"} |= "ERROR" [5m])) > 10
    duration: 5m
    annotations:
      summary: "High error rate detected"
      description: "Error rate is {{ $value }} errors/min"
    
  - name: slow_queries
    query: |
      avg({job="logcaster"} | json | response_time_ms > 1000 [5m]) > 100
    duration: 10m
    annotations:
      summary: "Slow queries detected"
      
  - name: security_events
    query: |
      {job="logcaster"} |~ "unauthorized|forbidden|injection|malicious"
    duration: 1m
    annotations:
      summary: "Security event detected"
      severity: critical
```

### 5.2 ë¡œê·¸ ì§‘ê³„ ëŒ€ì‹œë³´ë“œ

```json
{
  "dashboard": {
    "title": "LogCaster Log Analytics",
    "panels": [
      {
        "title": "Log Volume",
        "targets": [{
          "expr": "sum(rate({job=\"logcaster\"}[5m])) by (level)"
        }]
      },
      {
        "title": "Error Logs",
        "targets": [{
          "expr": "{job=\"logcaster\", level=\"ERROR\"}"
        }]
      },
      {
        "title": "Response Time Distribution",
        "targets": [{
          "expr": "histogram_quantile(0.95, sum(rate({job=\"logcaster\"} | json | __error__=\"\" | response_time_ms > 0 [5m])) by (le))"
        }]
      },
      {
        "title": "Top Errors",
        "targets": [{
          "expr": "topk(10, count by (message) ({job=\"logcaster\", level=\"ERROR\"}))"
        }]
      }
    ]
  }
}
```

---

## 6. ë¡œê·¸ ë³´ì¡´ ë° ì•„ì¹´ì´ë¹™

### 6.1 ë¡œê·¸ ë¡œí…Œì´ì…˜

```bash
# /etc/logrotate.d/logcaster
/var/log/logcaster/*.log {
    daily
    rotate 30
    compress
    delaycompress
    missingok
    notifempty
    create 0644 logcaster logcaster
    sharedscripts
    postrotate
        # ì‹œê·¸ë„ ì „ì†¡ìœ¼ë¡œ ë¡œê·¸ íŒŒì¼ ì¬ì˜¤í”ˆ
        kill -USR1 $(cat /var/run/logcaster.pid) 2>/dev/null || true
        
        # S3ì— ì•„ì¹´ì´ë¸Œ ì—…ë¡œë“œ
        for file in /var/log/logcaster/*.gz; do
            aws s3 cp $file s3://logcaster-archive/logs/$(date +%Y/%m/%d)/
            rm $file
        done
    endscript
}
```

### 6.2 ë¡œê·¸ ì•„ì¹´ì´ë¹™ ìŠ¤í¬ë¦½íŠ¸

```bash
#!/bin/bash
# archive-logs.sh

ARCHIVE_DIR="/archive/logs"
S3_BUCKET="logcaster-logs-archive"
RETENTION_DAYS=90

echo "ğŸ“¦ ë¡œê·¸ ì•„ì¹´ì´ë¹™ ì‹œì‘..."

# 1. ë¡œì»¬ ì•„ì¹´ì´ë¸Œ ìƒì„±
find /var/log/logcaster -name "*.log" -mtime +7 | while read file; do
    ARCHIVE_NAME=$(basename $file .log)_$(date -r $file +%Y%m%d).tar.gz
    tar -czf $ARCHIVE_DIR/$ARCHIVE_NAME $file
    rm $file
done

# 2. S3 ì—…ë¡œë“œ
aws s3 sync $ARCHIVE_DIR s3://$S3_BUCKET/archives/ \
    --storage-class GLACIER \
    --delete

# 3. ì˜¤ë˜ëœ ì•„ì¹´ì´ë¸Œ ì‚­ì œ
find $ARCHIVE_DIR -name "*.tar.gz" -mtime +$RETENTION_DAYS -delete

# 4. Elasticsearch ì¸ë±ìŠ¤ ì •ë¦¬
curl -X DELETE "elasticsearch:9200/logcaster-$(date -d '90 days ago' +%Y.%m.%d)"

echo "âœ… ë¡œê·¸ ì•„ì¹´ì´ë¹™ ì™„ë£Œ"
```

---

## 7. ë¡œê·¸ ë³´ì•ˆ

### 7.1 ë¯¼ê° ì •ë³´ ë§ˆìŠ¤í‚¹

```c
// log_masking.c
char* mask_sensitive_data(const char* message) {
    char* masked = strdup(message);
    
    // íŒ¨í„´ ì •ì˜
    struct {
        const char* pattern;
        const char* replacement;
    } patterns[] = {
        {"password=([^\\s]+)", "password=***"},
        {"token=([^\\s]+)", "token=***"},
        {"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b", "***@***.***"},
        {"\\b(?:\\d{4}[-\\s]?){3}\\d{4}\\b", "****-****-****-****"},  // ì¹´ë“œë²ˆí˜¸
        {"\\b\\d{3}-\\d{2}-\\d{4}\\b", "***-**-****"},  // SSN
        {NULL, NULL}
    };
    
    for (int i = 0; patterns[i].pattern; i++) {
        regex_t regex;
        regcomp(&regex, patterns[i].pattern, REG_EXTENDED);
        
        regmatch_t match;
        if (regexec(&regex, masked, 1, &match, 0) == 0) {
            // ë§ˆìŠ¤í‚¹ ì ìš©
            memset(masked + match.rm_so, '*', match.rm_eo - match.rm_so);
        }
        
        regfree(&regex);
    }
    
    return masked;
}
```

### 7.2 ë¡œê·¸ ì•”í˜¸í™”

```bash
#!/bin/bash
# encrypt-logs.sh

# ë¡œê·¸ ì•”í˜¸í™” (ì „ì†¡ ì¤‘)
openssl enc -aes-256-cbc -salt -in /var/log/logcaster/sensitive.log \
    -out /var/log/logcaster/sensitive.log.enc -k $LOG_ENCRYPTION_KEY

# ë¡œê·¸ ì„œëª… (ë¬´ê²°ì„± ë³´ì¥)
openssl dgst -sha256 -sign private_key.pem \
    -out /var/log/logcaster/sensitive.log.sig \
    /var/log/logcaster/sensitive.log
```

---

## 8. ì„±ëŠ¥ ìµœì í™”

### 8.1 ë¹„ë™ê¸° ë¡œê¹…

```c
// async_logger.c
#include <pthread.h>

typedef struct {
    log_entry_t* buffer;
    int capacity;
    int head;
    int tail;
    pthread_mutex_t mutex;
    pthread_cond_t not_empty;
    pthread_cond_t not_full;
} log_queue_t;

void* logger_thread(void* arg) {
    log_queue_t* queue = (log_queue_t*)arg;
    
    while (1) {
        pthread_mutex_lock(&queue->mutex);
        
        while (queue->head == queue->tail) {
            pthread_cond_wait(&queue->not_empty, &queue->mutex);
        }
        
        log_entry_t entry = queue->buffer[queue->head];
        queue->head = (queue->head + 1) % queue->capacity;
        
        pthread_cond_signal(&queue->not_full);
        pthread_mutex_unlock(&queue->mutex);
        
        // ì‹¤ì œ ë¡œê¹…
        log_structured(&entry);
    }
    
    return NULL;
}

void async_log(log_queue_t* queue, log_entry_t* entry) {
    pthread_mutex_lock(&queue->mutex);
    
    int next_tail = (queue->tail + 1) % queue->capacity;
    while (next_tail == queue->head) {
        pthread_cond_wait(&queue->not_full, &queue->mutex);
    }
    
    queue->buffer[queue->tail] = *entry;
    queue->tail = next_tail;
    
    pthread_cond_signal(&queue->not_empty);
    pthread_mutex_unlock(&queue->mutex);
}
```

---

## âœ… ì²´í¬ë¦¬ìŠ¤íŠ¸

### í•„ìˆ˜ í™•ì¸ì‚¬í•­
- [ ] êµ¬ì¡°í™”ëœ ë¡œê¹… êµ¬í˜„
- [ ] ë¡œê·¸ ìˆ˜ì§‘ ì—ì´ì „íŠ¸ ì„¤ì¹˜
- [ ] ì¤‘ì•™ ë¡œê·¸ ì €ì¥ì†Œ êµ¬ì„±
- [ ] ë¡œê·¸ ë¡œí…Œì´ì…˜ ì„¤ì •
- [ ] ë¯¼ê° ì •ë³´ ë§ˆìŠ¤í‚¹
- [ ] ë¡œê·¸ ë³´ì¡´ ì •ì±… ìˆ˜ë¦½

### ê¶Œì¥ì‚¬í•­
- [ ] ë¡œê·¸ ë¶„ì„ ëŒ€ì‹œë³´ë“œ
- [ ] ì‹¤ì‹œê°„ ì•Œë¦¼ ì„¤ì •
- [ ] ë¡œê·¸ ì•„ì¹´ì´ë¹™
- [ ] ì•”í˜¸í™” ì ìš©
- [ ] ë¹„ë™ê¸° ë¡œê¹…
- [ ] ë¡œê·¸ ê²€ìƒ‰ ìµœì í™”

---

## ğŸ¯ ë‹¤ìŒ ë‹¨ê³„
â†’ [07_security_checklist.md](07_security_checklist.md) - ë³´ì•ˆ ì²´í¬ë¦¬ìŠ¤íŠ¸